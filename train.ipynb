{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 1.3.3\n",
      "Numpy version: 1.20.1\n",
      "MatplotLib version: 3.5.3\n",
      "Sklearn version: 0.24.2\n",
      "Seaborn version: 0.12.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\");\n",
    "\n",
    "print('Pandas version:', pd.__version__);\n",
    "print('Numpy version:', np.__version__);\n",
    "print('MatplotLib version:', mpl.__version__);\n",
    "print('Sklearn version:', sklearn.__version__);\n",
    "print('Seaborn version:', sns.__version__);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the datasets for train and test\n",
    "\n",
    "The dataset should contain all columns used as predictors the terminal states of the compounds as integers in increasing order (column name here: \"Terminal State(int)\").\n",
    "\n",
    "### The following lists need to be specified:\n",
    "\n",
    "    global_features: list of all column names used as predictors\n",
    "\n",
    "    features_to_be_normed: list of all column names that should be scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train, val and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = TO_BE_SET;\n",
    "df_val = TO_BE_SET;\n",
    "\n",
    "df_test = TO_BE_SET;\n",
    "df_train_final = TO_BE_SET;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Scaling the initial train and val ...\");\n",
    "\n",
    "sc = StandardScaler()\n",
    "train_scaler = sc.fit(df_train[features_to_be_normed]);\n",
    "df_train[features_to_be_normed] = train_scaler.transform(df_train[features_to_be_normed]);\n",
    "df_val[features_to_be_normed] = train_scaler.transform(df_val[features_to_be_normed]);\n",
    "\n",
    "print(\"Scaling the train and test ...\");\n",
    "sc = StandardScaler()\n",
    "final_train_scaler = sc.fit(df_train_final[features_to_be_normed]);\n",
    "import pickle\n",
    "scalerfile = 'scaler_onlyassays.sav'\n",
    "pickle.dump(final_train_scaler, open(scalerfile, 'wb'))\n",
    "final_train_scaler = pickle.load(open(scalerfile, 'rb'))\n",
    "\n",
    "df_train_final[features_to_be_normed] = final_train_scaler.transform(df_train_final[features_to_be_normed]);\n",
    "df_test[features_to_be_normed] = final_train_scaler.transform(df_test[features_to_be_normed]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    " \n",
    "if torch.cuda.is_available():\n",
    "    print('use GPU');\n",
    "    device='cuda';\n",
    "else:\n",
    "    print('use CPU');\n",
    "    device='cpu';\n",
    "    \n",
    "from coral_pytorch.losses import corn_loss\n",
    "\n",
    "mod = importlib.import_module('Predictor_onlyGlobalFeats_wdropout');\n",
    "importlib.reload(mod);\n",
    "Predictor_onlyGlobalFeats_wdropout = getattr(mod, 'Predictor_onlyGlobalFeats_wdropout');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data( global_features, labels):\n",
    "\n",
    "    bPKs= [];\n",
    "    global_feat_list = [];\n",
    "\n",
    "    for i, smi in enumerate(labels):\n",
    "        \n",
    "        bPKs.append(int(labels[i]));\n",
    "        global_feat_list.append(torch.from_numpy(global_features[i,:])); \n",
    "        \n",
    "    return  global_feat_list, bPKs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(sample):\n",
    "    global_feats, labels = map(list,zip(*sample));\n",
    "\n",
    "    global_feats = torch.stack([torch.tensor(tmp) for tmp in global_feats]);\n",
    "\n",
    "    return  global_feats, torch.tensor(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precentile_borders(data, num_ints):\n",
    "    borders = [];\n",
    "    print(\"Interval borders:\");\n",
    "    percs = 100/num_ints;\n",
    "    \n",
    "    for tmp_int in range(num_ints-1):\n",
    "        tmp_perc = np.percentile(data, (tmp_int+1)*percs);\n",
    "        borders.append(tmp_perc);\n",
    "        print(tmp_perc);\n",
    "    \n",
    "    return borders;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gat(params):    \n",
    "   \n",
    "    print(params);\n",
    "    num_tasks = 5;\n",
    "    model = Predictor_onlyGlobalFeats_wdropout( global_feats=len(global_features), predictor_dropout=params[\"dropout\"], num_layers=params[\"depth\"],\n",
    "                                       n_tasks=num_tasks, predictor_hidden_feats=params[\"predictor_hidden_feats\"]);\n",
    "    \n",
    "    model = model.to(device);\n",
    "\n",
    "    global_feat_train, bPKs_train = get_data( df_train[global_features].to_numpy(dtype=np.float32), \n",
    "                                                                 df_train[\"Terminal State(int)\"].to_list());\n",
    "    global_feat_val, bPKs_val = get_data( df_val[global_features].to_numpy(dtype=np.float32),\n",
    "                                                           df_val[\"Terminal State(int)\"].to_list());\n",
    "\n",
    "    train_data = list(zip( global_feat_train, bPKs_train)).copy();\n",
    "    train_loader = DataLoader(train_data, batch_size=int(params[\"batch_size\"]), shuffle=True, collate_fn=collate, pin_memory=True, num_workers=1);\n",
    "    \n",
    "    val_data = list(zip( global_feat_val, bPKs_val)).copy();\n",
    "    val_loader = DataLoader(val_data, batch_size=256, shuffle=False, collate_fn=collate, drop_last=False);\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=params[\"weight_decay\"]);\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01,mode=\"triangular2\",cycle_momentum=False, step_size_up=2*len(train_loader));\n",
    "\n",
    "    #######################\n",
    "    ### Train the model ###\n",
    "    #######################\n",
    "    epoch_losses = [];\n",
    "    epoch_aucs = [];\n",
    "    \n",
    "    #early stopping params\n",
    "    num_no_improvements = 0;\n",
    "    best_val = 0;\n",
    "    patience = 10;\n",
    "    \n",
    "    for epoch in range(1,100):\n",
    "        model.train();\n",
    "        epoch_loss = 0;\n",
    "        epoch_acc = 0;\n",
    "        for i, (global_feats, labels) in enumerate(train_loader):\n",
    "\n",
    "            labels = labels.to(device);\n",
    "            global_feats = global_feats.to(device);\n",
    "\n",
    "            logits, _ = model( global_feats);\n",
    "            loss = corn_loss(logits, labels.float(), num_classes=num_tasks);\n",
    "            \n",
    "            optimizer.zero_grad();\n",
    "            loss.backward();\n",
    "            optimizer.step();\n",
    "            epoch_loss += loss.detach().item();\n",
    "            scheduler.step();\n",
    "            \n",
    "        epoch_losses.append(epoch_loss);\n",
    "\n",
    "        #######################\n",
    "        ### Valid the model ###\n",
    "        #######################\n",
    "        model.eval();\n",
    "        probs = [];\n",
    "        for i, ( global_feats, labels) in enumerate(val_loader):\n",
    "            global_feats = global_feats.to(device);\n",
    "            labels = labels.to(device);\n",
    "            logits, prob = model( global_feats);\n",
    "            tmp_probs = prob.cpu().detach().numpy();    \n",
    "            probs = probs + list(tmp_probs[:,1]);\n",
    "        \n",
    "        fpr, tpr, _ = metrics.roc_curve(df_val[\"bPK\"].to_list(), probs);\n",
    "        tmp_val = metrics.auc(fpr, tpr);\n",
    "        epoch_aucs.append(tmp_val);\n",
    "        \n",
    "        print(optimizer.param_groups[0]['lr']);\n",
    "\n",
    "        \n",
    "        if tmp_val > best_val:\n",
    "            num_no_improvements = 0;\n",
    "            best_val = tmp_val;\n",
    "        else:\n",
    "            num_no_improvements = num_no_improvements + 1;\n",
    "            if num_no_improvements>patience:\n",
    "                print(\"Early stopping here ...\");\n",
    "                break;\n",
    "        \n",
    "        epoch_aucs.append(tmp_val);\n",
    "        print(tmp_val);\n",
    "        \n",
    "    print(\"Best val: \" + repr(best_val));\n",
    "    return 1-best_val;\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "def fit_gat_final(params):\n",
    "    import copy\n",
    "    \n",
    "    num_tasks = 5;\n",
    "    print(params);\n",
    "    model = Predictor_onlyGlobalFeats_wdropout( global_feats=len(global_features), predictor_dropout=params[\"dropout\"], num_layers=params[\"depth\"],\n",
    "                                       n_tasks=num_tasks, predictor_hidden_feats=params[\"predictor_hidden_feats\"]);\n",
    "\n",
    "    model = model.to(device);\n",
    "\n",
    "    global_feat_train, bPKs_train = get_data( df_train_final[global_features].to_numpy(dtype=np.float32), \n",
    "                                                                 df_train_final[\"Terminal State(int)\"].to_list());\n",
    "    \n",
    "    global_feat_val, bPKs_val = get_data( df_test[global_features].to_numpy(dtype=np.float32),\n",
    "                                                           df_test[\"Terminal State(int)\"].to_list());\n",
    "    print(\"Data loaded ...\");\n",
    "    \n",
    "    train_data = list(zip( global_feat_train, bPKs_train)).copy();\n",
    "    train_loader = DataLoader(train_data, batch_size=int(params[\"batch_size\"]), shuffle=True, collate_fn=collate, pin_memory=True, num_workers=1);\n",
    "    \n",
    "    val_data = list(zip( global_feat_val, bPKs_val)).copy();\n",
    "    val_loader = DataLoader(val_data, batch_size=256, shuffle=False, collate_fn=collate, drop_last=False);\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=params[\"weight_decay\"]);\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01, mode=\"triangular2\", step_size_up=2*len(train_loader),cycle_momentum=False);\n",
    "    model.train();\n",
    "\n",
    "    epoch_losses = [];\n",
    "    epoch_accuracies = [];\n",
    "    \n",
    "    #early stopping params\n",
    "    num_no_improvements = 0;\n",
    "    best_val = 0;\n",
    "    patience = 10;\n",
    "    \n",
    "    \n",
    "    for epoch in range(1,100):\n",
    "        model.train();\n",
    "        epoch_loss = 0;\n",
    "        for i, (global_feats, labels) in enumerate(train_loader):\n",
    "\n",
    "            labels = labels.to(device);\n",
    "            global_feats = global_feats.to(device);\n",
    "            \n",
    "\n",
    "            logits, probas = model( global_feats);\n",
    "            loss = corn_loss(logits, labels.float(), num_classes=num_tasks);\n",
    "            \n",
    "            optimizer.zero_grad();\n",
    "            loss.backward();\n",
    "            optimizer.step();\n",
    "            epoch_loss += loss.detach().item();\n",
    "            scheduler.step();\n",
    "      \n",
    "        epoch_loss /= (i + 1)\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"epoch: {epoch}, LOSS: {epoch_loss:.3f}\");\n",
    "        epoch_losses.append(epoch_loss);\n",
    "        \n",
    "        #######################\n",
    "        ### Valid the model ###\n",
    "        #######################\n",
    "        model.eval();\n",
    "        probs = [];\n",
    "        for i, ( global_feats, labels) in enumerate(val_loader):            \n",
    "            labels = labels.to(device);\n",
    "            global_feats = global_feats.to(device);\n",
    "            _, prob = model(global_feats);\n",
    "            tmp_probs = prob.cpu().detach().numpy();\n",
    "            probs = probs + list(tmp_probs[:,1]);\n",
    "            \n",
    "        fpr, tpr, _ = metrics.roc_curve(df_test[\"bPK\"].to_list(), probs);\n",
    "        tmp_val = metrics.auc(fpr, tpr);\n",
    "        print(\"Learning rate: \" + str(optimizer.param_groups[0]['lr']));\n",
    "\n",
    "        if tmp_val > best_val:\n",
    "            num_no_improvements = 0;\n",
    "            best_val = tmp_val;\n",
    "            best_model = copy.deepcopy(model);\n",
    "        else:\n",
    "            num_no_improvements = num_no_improvements + 1;\n",
    "            if num_no_improvements>patience:\n",
    "                print(\"Early stopping here ...\");\n",
    "                break;\n",
    "        \n",
    "        epoch_accuracies.append(tmp_val);\n",
    "        print(tmp_val);\n",
    "      \n",
    "    print(\"Best val: \" + repr(best_val));\n",
    "    return best_model, best_val;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "final_params = {'batch_size': 32, 'depth': 7, 'dropout': 0.3, 'predictor_hidden_feats': 256, 'weight_decay': 0.01};\n",
    "\n",
    "auc_threshold = 0.5;\n",
    "best_auc = 0;\n",
    "i = 0;\n",
    "while i < 10:\n",
    "    model, auc = fit_gat_final(final_params);\n",
    "    if auc>auc_threshold:\n",
    "        torch.save(model.state_dict(), \"weights_\" + str(i) + \".pth\");\n",
    "        i = i + 1;\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc;\n",
    "        best_model = copy.deepcopy(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import predict_onlyassays\n",
    "mod = importlib.reload(predict_onlyassays);\n",
    "\n",
    "df_test = df.iloc[int(0.75*df.shape[0]): ,:];\n",
    "preds = mod.predict(df_test.copy(), \".\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(df_test[\"bPK\"], preds);\n",
    "plt.plot(fpr, tpr, color=\"black\", linewidth=2);\n",
    "plt.plot(np.arange(0, 1, 0.01), np.arange(0,1, 0.01), color=\"red\", linewidth=0.5);\n",
    "plt.xlabel(\"False positive rate\", fontsize=12);\n",
    "plt.ylabel(\"True positive rate\", fontsize=12);\n",
    "print(\"AUC\");\n",
    "print(metrics.auc(fpr, tpr));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = metrics.precision_recall_curve(df_test[\"bPK\"], preds);\n",
    "\n",
    "f, axs = plt.subplots(1,3, figsize=(17,5));\n",
    "\n",
    "axs[0].plot(recall[1:-1], precision[1:-1], color=\"black\", linewidth=2);\n",
    "axs[0].set_xlabel(\"Recall\");\n",
    "axs[0].set_ylabel(\"Precision\");\n",
    "#axs[0].set_yscale(\"log\");\n",
    "\n",
    "axs[1].plot(thresholds, precision[:-1], color=\"black\", linewidth=2);\n",
    "axs[1].set_xlabel(\"Threshold\");\n",
    "axs[1].set_ylabel(\"Precision\");\n",
    "\n",
    "axs[2].plot(thresholds, recall[:-1], color=\"black\", linewidth=2);\n",
    "axs[2].set_xlabel(\"Threshold\");\n",
    "axs[2].set_ylabel(\"Recall\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "normed_preds = preds - np.min(preds);\n",
    "normed_preds = normed_preds/np.max(normed_preds);\n",
    "\n",
    "bPKs = df_test[\"bPK\"].to_numpy();\n",
    "prob_true, prob_pred = calibration_curve(bPKs, preds, n_bins=5, strategy=\"quantile\");\n",
    "\n",
    "y_tests_np = np.array(bPKs);\n",
    "baseline_prob = len(y_tests_np[y_tests_np==1.0])/len(y_tests_np);\n",
    "print(\"Baseline probability:\" + repr(baseline_prob));\n",
    "\n",
    "borders = calc_precentile_borders(preds, 5);\n",
    "\n",
    "sns.regplot(x=prob_pred, y=prob_true, ci=0, order=0, color=\"black\", lowess=True);\n",
    "plt.xlabel(\"bPK score\");\n",
    "plt.ylabel(\"Probabilit of transition beyond PK\");"
   ]
  }
 ],
 "metadata": {
  "@deathbeds/ipydrawio": {
   "xml": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
